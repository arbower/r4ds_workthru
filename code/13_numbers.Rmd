---
title: "r4ds_ch13_numbers"
author: "Andy B. PhD"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document: 
    toc: true
    toc_depth: 2
    toc_float: true
    code_folding: hide
    theme: spacelab
---

# Summary

Numeric vectors are the backbone of data science. This chapter will systematically review what all you can do with them. 

```{r}
pacman::p_load(tidyverse, nycflights13)

flights <- nycflights13::flights
```

# Making Numbers

In most cases you will have a vector of `integers` or `double` numbers. In SOME cases (often in ways that won't make sense) you will get strings that represent numbers. `readr` provides two useful functions for parsing strings into numbers: `parse_double()` and `parse_number()`. 

```{r}
x <- c("1.2", "5.6", "1e3")

parse_double(x)
```

You use `parse_number()` when you have strings that contain non-numeric values/text (e.g., currency).

```{r}
x <- c("$1,234", "USE 3,513", "59%")

parse_number(x)
```

# Counts

It's surprising how much data science you can do with just counts and a little arithmetic. 

```{r}
flights |> 
  count(dest, sort = TRUE)
```

You can perform the same computation "by hand" with group_by(), summarize(), and n(), which can be more powerful when you want to computer other summaries at the same time.

```{r}
flights |> 
  group_by(dest) |> 
  summarize(
    n = n(),
    delay = mean(arr_delay, na.rm = TRUE)
  )
```

`n()` is a special summary function that doesn't take any arguments and instead assess information about the "current" group. 

There are a couple of variants of `n()` and `count()`

`n_distinct()` counts the number of distinct (unique) values of one or more variables. For example, we could figure out which destinations are served by the most carriers:

```{r}
flights |> 
  group_by(dest) |> 
  summarize(
    carriers = n_distinct(carrier)
  ) |> 
  arrange(
    desc(carriers)
  )

```

`sum()` is like a weighted count. 

```{r}
flights |> 
  group_by(tailnum) |> 
  summarize(
    miles = sum(distance)
  )
```

Weighted counts are a common problem, so `count()` has a `wt` argument that does the same thing:

```{r}
flights |> 
  count(tailnum, wt = distance)
```

Very useful, you can combine `sum()` and `is.na()` to count missing variables. In the flights dataset these missing values represent cancelled flights

```{r}
flights |> 
  group_by(dest) |> 
  summarize(
    n_cancelled = sum(is.na(dep_time))
  )
```

#### Exercises

### 13.1a How can you use count() to count the number of rows with missing value for a given variable?

```{r}
flights |> 
  count(is.na(dep_delay))
```

### 13.2a Expand teh following calls to count() to instead use group_by(), summarize(), and arrange().

```{r}
flights |> 
  group_by(dest) |> 
  summarize(
    n = n(),
  ) |> 
  arrange(desc(n))

flights |> 
  group_by(tailnum) |> 
  summarize(
    n = n(),
    total = sum(distance)
  ) |> 
  arrange(desc(total))
```

# Numeric Transformations

Transformations work well with `mutate()` because their output is the same length as the input. 

## Arithemtic and Recycling Rules

What happens when there is a longer and shorter vector? R recycles across the shorter vector (i.e., repeating across the short vector)

```{r}
x <- c(1, 2, 10, 20)

x / 5 #repeats this

# r will give a warning when there are more than one recycled value

x * c(1, 2) # but not always

x * c(1, 2, 3)
```

The recycling rules are also applied to logical comparisons and can lead to a surprising result if you accidentally use `==` instead of `%in%` and the data frame has an unfortunate number of rows. 

```{r}
flights |> 
  filter(month == c(1, 2)) # odd number rows of January flights and even number rows of February, with no warning
```

## Minimum and Maximum

The arithmetic functions work with pairs of variables. Two closely related functions are pmin() and pmax(), which when given two or more variables will return the smallest or largest value in each row. 

```{r}
df <- tribble(
  ~x, ~y,
  1, 3,
  5, 2,
  7, NA
)

df

# you can tell you've used the wrong one when the wrong values are returned, note here how they are all the same
df |> 
  mutate(
    min = min(x, y, na.rm = TRUE),
    max = max(x, y, na.rm = TRUE)
  )

# this is better
df |> 
  mutate(
    min = pmin(x, y, na.rm = TRUE),
    max = pmax(x, y, na.rm = TRUE)
  )

```

## Modular Arithmetic

Modular arithmetic is the type of math you learned before decimals. 

```{r}
1:10 %/% 3

1:10 %% 3
```

This is handy in some cases because we an unpack numbers that were inputed for ease, but not for analyzing, like the sched_dep_time variable. 

```{r}
flights |> 
  mutate(
    hour = sched_dep_time %/% 100,
    minute = sched_dep_time %% 100,
    .keep = "used"
    
  )

```

We can combine that with the `mean(is.na(x))` trick from the `summarize()` to see how the proportion of cancelled flights varies over the course of the day. 

```{r}
flights |> 
  group_by(hour = sched_dep_time %/% 100) |> 
  summarize(
    prop_cancelled = mean(is.na(dep_time)),
    n = n()
  ) |> 
  filter(hour > 1) |> 
  ggplot(aes(x = hour, y = prop_cancelled)) +
  geom_line(color = "grey44") + 
  geom_point(aes(size = n))

```

## Logarithms

Logarithms are incredibly useful transformations where data ranges over many orders of magnitude and for converting exponential growth to linear growth. 

* `log()` the base e
* `log2()` where a change of 1 on the log scale corresponds to a doubling on the original scale
* `log10()` where 3 is equivalent with 3*10^3, 3e3. 

* You can take `exp()` to get the exponentiated form of `log()`
* Likewise 2^ and 10^ for log2() and log10() respectively. 

## Rounding

`round(x)` will round a number to the nearest #. However, round uses "Banker's" rounding: if a number is halfway between two integers it will be rounded to the even integer. So, half of 0.5 will be rounded up, and half will be rounded down. 

```{r}
round(1234.56)

round(123.456, 2)

round(123.456, -1)

round(123.456, -2)
```

`round()` and be paired with `floor()` and `ceiling()`to always round up or down. 

```{r}
x <- 123.456

floor(x)

ceiling(x)

# they don't have digits so you can do some math

floor(x / 0.01) * 0.01

ceiling(x / 0.01) * 0.01
```

You can use the same strategy as above if you want `round()` to round to the nearst multiple of a number.

```{r}
round(x / 4) * 4

round(x / 0.25) * 0.25
```

## Cutting Numbers into Ranges

Use `cut()` i.e., bin, to break up a numeric vector into discrete buckets. 

```{r}
x <- c(1, 2, 4, 10, 15, 20)

cut(x, breaks = c(0, 5, 10, 15, 20))

# they don't need to be evenly spaced
cut(x, breaks = c(0, 5, 10, 100))
```

You can optimally supply your own labels to each level of break, note, you should _have one less labels than breaks_ which is new for me. 

```{r}
cut(x, 
    breaks = c(0, 5, 10, 15, 20),
    labels = c("sm", "md", "lg", "xl")
)

```

Note, any value outside of your specified range will be transformed to NA

```{r}
y <- c(NA, -10, 5, 10, 30)

cut(y, breaks = c(0, 5, 10, 15, 20))
```

## Cumulative and Rolling Aggregates

Base r provides `cumsum()`, `cumprod()`, `cummin()`, and `cummax()` for running, or cumulative, sums, products, mins and maxes. `dplyr` provides `cummean()` for cumulative means. 

```{r}
x <- 1:100

cumsum(x)

cumprod(x)

cummin(x)

cummax(x)

cummean(x)
```

#### Exercises

### 12.1b Explain in words what each line of code used to generate Figure 13-1 does:

```{r}
flights |> #data
  group_by(hour = sched_dep_time %/% 100) |> #grouped by sched_dep_time hour
  summarize(
    prop_cancelled = mean(is.na(dep_time)),
    n = n()
  ) |> # mean time departure when missing with the number of missing dep_time
  filter(hour > 1) |> # hour is greater than 1am
  ggplot(aes(x = hour, y = prop_cancelled)) + # plot x and y aesthetics
  geom_line(color = "grey44") +  # as a line that is grey
  geom_point(aes(size = n)) # with pints that are adjusted to the size of number of cancelled flights
```

### 12.2b What trigometric functios does R provide? Guess some names and look up the documentation. Do they use degrees or radians? 

```{r}
??arcsin # all in radians

```

### 12.3b Currently dep_time and sched_dep_time are conenient to look at but hard to compute with because they're not really continuous numbers. You can see the basic problem by running the following code: there's a gap between each hour:

```{r}
flights |> 
  filter(month == 1, day == 1) |> 
  ggplot(aes(x = sched_dep_time, y = dep_delay)) +
  geom_point()

flights |> 
  mutate(
    hour = sched_dep_time %/% 100 * 60,
    minute = sched_dep_time %% 100,
    time_min = hour + minute,
    sched_dep_time_updated = (24 * 60) - time_min,
    .keep = "used"
  )
```

### 12.4b Round dep_time and arr_time to the nearest five minutes

```{r}
flights |> 
  mutate(
    dep_time_5 = ceiling(dep_time / 5) * 5,
    arr_time_5 = ceiling(arr_time / 5) * 5,
    .keep = "used"
  )

```

# General Transformations

The following are general transformation we see often in the data science world. 

## Ranks

`dplyr` provides a number of ranking functions inspired by SQL, but you should always start with `dplyr::min_rank()` it uses the typical method for dealing with ties (i.e., 1, 2, 2, 4)

```{r}
x <- c(1, 2, 2, 3, 4, NA)

min_rank(x)

# or

min_rank(desc(x))
```

If that doesn't give you what you need, then use dplyr::row_number(), ::dense_rank(), ::percent_rank(), and ::cume_dist()

```{r}
df <- tibble(x = x)

df |> 
  mutate(
    row_number = row_number(x),
    dense_rank = dense_rank(x),
    percent_rank = percent_rank(x),
    cume_dist = cume_dist(x)
  )
```
You can specifiy arguments within each to achieve similar restults with ties.method() and na.last = "keep" to keep NAs. 

You can also use row_number within a mutate call to give you the power to divide data into similarly sized groups. 

```{r}
df <- tibble(id = 1:12)

df |> 
  mutate(
    row0 = row_number() - 1,
    three_groups = row0 %% 3,
    three_in_each_group = row0 %/% 3
  )
```

## Offsets

`dplyr::lead()` and `dplyr::lag()` allow you to refer to the values just before or just after the "current" value. They pad NAs at the start or end respectively. 

```{r}
x <- c(2, 5, 11, 11, 19, 35)

lag(x)
lead(x)

x - lag(x) # gives you the difference between the current and previous value
x == lag(x) # tells you when the current value changes
# you can lead or lag by however many positions your heart desires
```

## Consecutive Identifiers

Sometimes you want to start a new group everytime some event occurs. Like start a new session after some x# of minutes. Or an at bat with some other change in identifier.

```{r}
events <- tibble(
  time = c(0, 1, 2, 3, 5, 10, 12, 15, 17, 19, 20, 27, 28, 30)
)
```

You've computed the time between each event and figured out if there's a gap that's big enough to qualify. 

```{r}
events <- events |> 
  mutate(
    diff = time - lag(time, default = first(time)),
    has_gap = diff >= 5
  )
```

But, how do we go from a logical vector to something we can `group_by`? `cumsum()` comes to the rescue. As gap = TRUE, will increment group by one. 

```{r}
events |> 
  mutate(
    group = cumsum(has_gap)
  )
```

Another approach for creating grouping variables in `consecutive_id()`, which starts a new group every time one of its arguments changes. 

Imagine you have a dataframe with a bunch of repeated values
```{r}
df <- tibble(
  x = c("a", "a", "a", "b", "c", "c", "d", "e", "a", "a", "b", "b"),
  y = c(1, 2, 3, 2, 4, 1, 3, 9, 4 ,8, 10, 199)
)

df |> 
  group_by(id = consecutive_id(x)) |> 
  slice_head(n = 1)
```

#### Exercises

### 12.1c Find the 10 most delayed flights using a ranking function. How do you want to handle ties? Carefully read the documentation for min_rank()

```{r}
flights |> 
  filter(dep_delay > 60) |> 
  mutate(delay_rank = min_rank(dep_delay),
         .before = year
  ) |> 
  arrange(desc(delay_rank)) |> 
  head(n = 10) 
```

### 12.2c Which plane tailnum has the worst on-time record?

```{r}
flights |> 
  filter(dep_delay > 60) |> 
  group_by(tailnum) |> 
  mutate(delay_mean = mean(dep_delay, na.rm = TRUE),
         mean_rank = min_rank(delay_mean),
         n_flights = n(),
         .before = year
  ) |> 
  filter(n_flights >= 10) |> 
  arrange(desc(dep_delay)) |> 
  head(n = 10) 
```

### 12.3c What time of day should you fly to avoid delays? 
```{r}
flights |> 
  group_by(hour = sched_dep_time %/% 100) |> 
  summarize(
    prop_delayed = mean(is.na(dep_delay)),
    n = n()
  ) |> 
  filter(hour > 1) |> 
  ggplot(aes(x = hour, y = prop_delayed)) +
  geom_line(color = "grey44") + 
  geom_point(aes(size = n)) 

```

### 12.4c What does the following code do? 

```{r}
flights |> 
  group_by(dest) |> 
  filter(row_number() < 4) # filters the flights that have less than 4 flights

flights |> 
  group_by(dest) |> 
  filter(row_number(dep_delay) < 4) #filters the flights that have delays by less than 4
```

### 12.5c For each destination, compute the total minutes of delay. For each flight, compute the proporation of total delay for its destination.

```{r}
flights |> 
  group_by(dest) |> 
  summarize(
    total_delay = sum(dep_delay, na.rm = TRUE),
    n = n(),
    avg_delay = round(total_delay / n, 0)
  ) |> 
  arrange(desc(total_delay))

flights |> 
  group_by(flight, dest) |> 
  summarize(
    total_delay = sum(dep_delay, na.rm = TRUE),
    n = n(),
    avg_delay = round(total_delay / n, 0)
  ) |> 
  arrange(flight, desc(total_delay))

```

### 12.6c Delays are temporally correlated: even once the problem that caused the initial delay has been resolved, later flights are delayed to allow earlier flights to leave. Using `lag()`, explore how the average flgith delay for an hour is related to the average delay for the previous hour? 

```{r}
flights |> 
  mutate(hour = dep_time %/% 100) |> 
  group_by(year, month, day, hour) |> 
  summarize(
    dep_delay = mean(dep_delay, na.rm = TRUE),
    n = n(),
    .groups = "drop"
  ) |> 
  filter(n > 5) |> 
  mutate(
    previous_hour = lag(dep_delay, order_by = hour)
  )
```

### 12.7c Look at each destination. Can you find flights that are suspiciously fast (i.e., flights that represent a potential data entry error)? Compute the air time of a flight relative to the shortest flight to that destination. Which flights were most delayed in the air? 

```{r}
flights %>%
  filter(!is.na(air_time)) %>%
  group_by(dest, origin) %>%
  mutate(
    air_time_mean = mean(air_time),
    air_time_sd = sd(air_time),
    n = n()
  ) %>%
  ungroup() %>%
  mutate(air_time_standard = (air_time - air_time_mean) / (air_time_sd + 1)) |> 
  arrange(air_time_standard) %>%
  select(
    carrier, flight, origin, dest, month, day,
    air_time, air_time_mean, air_time_standard
  ) %>%
  head(10) %>%
  print(width = Inf)


flights |> 
  group_by(origin, dest) |> 
  mutate(
    air_time_min = min(air_time, na.rm = TRUE),
    air_time_delay = air_time - air_time_min,
    air_time_delay_pct = air_time_delay / air_time_min * 100
  ) |> 
  arrange(desc(air_time_delay)) %>%
  select(
    air_time_delay, carrier, flight,
    origin, dest, year, month, day, dep_time,
    air_time, air_time_min
  ) %>%
  head() %>%
  print(width = Inf)

```

### 12.8c Find all destinations that are flown by at least two carriers. Use those destinations to come up with a relative ranking of the carriers based on their performance for the same destination. 

! These are really hard for some reason. Maybe long covid?

```{r}
flights |> 
  group_by(dest) |> 
  mutate(
    n_carriers = n_distinct(carrier)
  ) |> 
  filter(n_carriers > 1) |> 
  group_by(carrier, dest) |> 
  mutate(
    rank_delay = dense_rank(dep_delay),
    .keep = "used"
  ) |> 
  arrange(carrier, dest, rank_delay)

```

# Numeric Summaries

## Center

Mean and Median. 

```{r}
flights |> 
  group_by(year, month, day) |> 
  summarize(
    mean = mean(dep_delay, na.rm = TRUE),
    median = median(dep_delay, na.rm = TRUE),
    n = n(),
    .groups = "drop"
  ) |> 
  ggplot(aes(x = mean, y = median)) +
  geom_abline(slope = 1, intercept = 0, color = "firebrick1", linewidth = 2) + 
  geom_point()
```

## Minimum, Maximum, and Quantiles

What if you want to know the largest and smallest values, or further generalizations of the median, 25, 50, 75, 90, 95. 99% of the data is less than.  

```{r}
flights |> 
  group_by(year, month, day) |> 
  summarize(
    max = max(dep_delay, na.rm = TRUE),
    q95 = quantile(dep_delay, 0.95, na.rm = TRUE),
    .groups = "drop"
  )

```

## Spread

Variation around the central tendency. `sd()` and `IQR()` can be used to make sure airports are staying the same distance apart. 
```{r}
# Maybe EGE moved or altered runways in 2013
flights |> 
  group_by(origin, dest) |> 
  summarize(
    distance_sd = IQR(distance),
    n = n(),
    .groups = "drop"
  ) |> 
  filter(distance_sd > 0)
```

## Distributions

All of the numeric summaries are ways to reduce distributions to a single, or two numbers. Indeed, if you recall, you can recreate a normal distribution or summarize it, with the mean and sd. As such they are fundamentally reductive, and could lead to you missing some important findings. It's always a good idea to visualize the distribution to ensure your choice of center and spread are correct. It's also important, and something many folks forget, to evaluate the distributions for each sub-group to ensure that the same treatment can apply for each.

```{r}
flights |> 
  filter(dep_delay < 120) |> 
  ggplot(aes(x = dep_delay, group = interaction(day, month))) +
  geom_freqpoly(binwidth = 5, alpha = 1/5)

```

## Positions

Extracting the `first()` or `last()` or `nth()` value from a vector works for all vector types (i.e., numeric and categorical).

```{r}
flights |> 
  group_by(year, month, day) |> 
  summarize(
    first_dep = first(dep_time, na_rm = TRUE),
    last_dep = last(dep_time, na_rm = TRUE),
    fith_dep = nth(dep_time, 5, na_rm = TRUE)
  )

```

These work similar to the `[,]` functions in base r. So why do we need them? Well, they will work even if the specified position doesn't exist. order_by allows you to override locally the ordering of the data. na_rm removes missing. 

Extracting values at positions is sort of like filtering on ranks:

```{r}
# this finds the first and last flights within a day
flights |> 
  group_by(year, month, day) |> 
  mutate(
    r = min_rank(sched_dep_time)
  ) |> 
  filter(r %in% c(1, max(r)))

```

## with Mutate()

Because of the recycling rules you can do most of these same functions with mutate. This is useful when you want to do some sort of group standardization. 
* `x / sum(x)` calculates a proportion total
* `(x - mean(x)) / sd(x)` computes a z-score (standardized to mean 0 and standard deviation of 1)
* `(x - min(x)) / max(x) - min(x)` standardizes the range from 0 to 1
* `x / first(x)` computes an index based on teh first obseration (e.g., useful to calculate pitches in an at bat)

### Exercises

#### 12.1d Brainstorm at least five ways to assess the typical delay characteristics of a group of flights. When is mean() useful? When is median()? WHen might you want to use something else? Should you use arrival delay or departure delay? Why might you want to use data from planes? 

_Answer_ Mean, First, Last, Median, IQR/SD. Mean is useful when relatively normally distributed. Median is useful if a plane exploded or some other long delay. You might want to use something else if you were evaluating a storm's impact and maybe you'd like to see when the first or last flight was delayed to evaluated the impact of the storm on the flights from that destination that day. Youd should use both, and depends on your question. Some planes are faster than others. 

#### 12.2d Which destination shows the greatest variation of air speed? 

```{r}
flights |> 
  group_by(dest) |> 
  summarize(
    sd_airspeed = sd(air_time / distance, na.rm = TRUE)
  ) |> 
  arrange(desc(sd_airspeed))

```


#### 12.3d Create a plot to further explore the adventures of EGE airport. Can you find any evidence that the airport moved? Can you find another variable that might explain the difference? 

```{r}
flights |> 
  filter(dest == "EGE") |> 
  ggplot(aes(x = distance)) +
  geom_histogram() +
  facet_wrap(carrier ~ origin) #switched airlines, so flying to a different runway? maybe

```